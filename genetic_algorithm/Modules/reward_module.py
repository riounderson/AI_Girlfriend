# Modules/reward_module.py

from short_term_memory import STM
from Utils.internal_state import InternalState
from Modules.working_memory import WorkingMemory
from Utils.invoke_llm import get_claude_response

class RewardModule:
    def __init__(self):
        self.internal_state = InternalState()
        self.wm = WorkingMemory()
        self.stm = STM()

    def evaluate_reward(self, purpose: str = None):
        recent_memory = self.wm.get_memory()
        if not recent_memory or not purpose:
            print("âš ï¸ è©•ä¾¡å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆç›®çš„ã¾ãŸã¯è¨˜æ†¶ãŒç©ºï¼‰")
            return

        recent_texts = "\n".join([entry["text"] for entry in recent_memory[-3:]])

        prompt = f"""
        ã‚ãªãŸã¯ç›®çš„ã‚’æŒã£ã¦ç›¸æ‰‹ã¨ä¼šè©±ã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®ç›®çš„ã¨ç›´è¿‘ã®ä¼šè©±ã‚’åŸºã«ã€ç›®çš„é”æˆåº¦ï¼ˆ-1.0ã€œ1.0ï¼‰ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        **ãªãŠè©•ä¾¡ã®ç†ç”±ãªã©ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡ã„ã‚Šã¾ã›ã‚“ã€‚ç‚¹æ•°ã ã‘ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        ã€ç›®çš„ã€‘
        {purpose}

        ã€ä¼šè©±ã€‘
        {recent_texts}
        """

        try:
            response = get_claude_response(prompt)
            print(f"responseã§ãˆãˆã‚¨ã‚¹: {response}")
            reward_score = float(response.strip())
            reward_score = max(-1.0, min(1.0, reward_score))
        except Exception as e:
            print(f"âš ï¸ LLM è©•ä¾¡å¤±æ•—: {e}")
            reward_score = 0.0

        print(f"ðŸ† å ±é…¬ã‚¹ã‚³ã‚¢ï¼ˆç›®çš„é”æˆåº¦ï¼‰: {reward_score:.2f}")
        self.internal_state.modify_state_value("short_reward", reward_score)

        # â¬‡ï¸ é«˜å ±é…¬ã ã£ãŸå ´åˆã¯ STM ã«è¨˜æ†¶ã™ã‚‹
        if reward_score >= 0.7:
            self.stm.add_learning_result(
                purpose=purpose,
                feedback=recent_texts,
                reward=reward_score
            )
    
    def evaluate_longtime_reward(self):
        goal = self.internal_state.retrieve_text_state("long_term_goal")
        recent_memory = self.wm.get_memory()
        if not recent_memory or not goal:
            print("âš ï¸ è©•ä¾¡å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆç›®çš„ã¾ãŸã¯è¨˜æ†¶ãŒç©ºï¼‰")
            return

        recent_texts = "\n".join([entry["text"] for entry in recent_memory[-8:]])

        prompt = f"""
        ã‚ãªãŸã¯å¯¾äººé–¢ä¿‚ã«ãŠã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ãªé•·æœŸçš„ãªç›®æ¨™ã‚’ç­–å®šã—ã¦è¡Œå‹•ã‚’ã—ã¦ã„ã¾ã™ã€‚ç›´è¿‘ã®ä¼šè©±ã®å†…å®¹ã‹ã‚‰ç›®æ¨™é”æˆåº¦ï¼ˆ-1.0ã€œ1.0ï¼‰ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        ä¼šè©±å†…å®¹ã‹ã‚‰è¦‹ã¦å®Œå…¨ã«ç›®æ¨™ãŒé”æˆã•ã‚Œã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹å ´åˆ: 1.0
        ex) å¥½æ„ã‚’å¾—ãŸã„äººã‹ã‚‰ã€Œå¥½ãã ã€ã‚„å¥½æ„ã‚’ä¼ãˆã‚‰ã‚‹ã¨ããªã©
        ç‰¹ã«åˆ¤æ–­ã§ããªã„å ´åˆã¯0
        ex) ã€Œã“ã‚“ã«ã¡ã¯ã€ãªã©ã®å˜ãªã‚‹æ—¥å¸¸ä¼šè©±ã§ç‰¹ã«ä¼šè©±å†…å®¹ãŒé•·æœŸçš„ç›®æ¨™ã¨é–¢ä¿‚ã—ãªã„ã¨æ€ã‚ã‚Œã‚‹å ´åˆ
        å®Œå…¨ã«å¤±æ•—ã—ã¦ã„ã‚‹å ´åˆã¯-1.0
        ex) å¥½æ„ã‚’å¾—ãŸã„äººã‹ã‚‰æ•µæ„ã‚’å‘ã‘ã‚‰ã‚Œã‚‹ç™ºè¨€ã‚’ã•ã‚Œã‚‹ãªã©
        ã¨ã—ã¦ãã ã•ã„ã€‚

        ã€çµ¶å¯¾å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«ã€‘
        ãƒ»è©•ä¾¡ã®ç†ç”±ãªã©ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡ã„ã‚Šã¾ã›ã‚“ã€‚ç‚¹æ•°ã ã‘ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨

        ã€é•·æœŸçš„ãªç›®æ¨™ã€‘
        {goal}

        ã€ä¼šè©±ã€‘
        {recent_texts}
        """

        try:
            response = get_claude_response(prompt)
            long_reward_score = float(response.strip())
            long_reward_score = max(-1.0, min(1.0, long_reward_score))
        except Exception as e:
            print(f"âš ï¸ LLM è©•ä¾¡å¤±æ•—: {e}")
            long_reward_score = 0.0

        print(f"ðŸ† å ±é…¬ã‚¹ã‚³ã‚¢ï¼ˆç›®æ¨™é”æˆåº¦ï¼‰: {long_reward_score:.2f}")
        self.internal_state.modify_state_value("long_term_reward", long_reward_score)

        # â¬‡ï¸ é«˜å ±é…¬ã ã£ãŸå ´åˆã¯ STM ã«è¨˜æ†¶ã™ã‚‹
        if long_reward_score >= 0.7:
            self.stm.add_learning_result(
                purpose=goal,
                feedback=recent_texts,
                reward=long_reward_score
            )

    
