import torch
import numpy as np
from Utils.vae_model import VAE
import threading
from sentence_transformers import SentenceTransformer

class InternalState:
    """
    Internal State ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    æ•°å€¤æƒ…å ±ã¯ VAE ã«æ¸¡ã—ã¦æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è©•ä¾¡ã€
    ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã¯ãã®ã¾ã¾ä¿å­˜ãƒ»å‚ç…§ã•ã‚Œã‚‹ã€‚
    """
    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(InternalState, cls).__new__(cls)
        return cls._instance
    def __init__(self):
        self.vae = VAE(input_dim=5, latent_dim=5)
        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
        self.state_vector = torch.tensor([0.5, 0.5, 0.5, 0.5, 0.5], dtype=torch.float32).unsqueeze(0)

        # æ•°å€¤è©•ä¾¡ç”¨ã®çŠ¶æ…‹ï¼ˆVAEã«å…¥åŠ›ã™ã‚‹ï¼‰
        self.state_values = {
            "common_sense": 0.5,
            "recognition": 0.5,
            "short_reward": 0.5,
            "long_term_reward": 0.5,
            "sociality": 0.5
        }

        # æ„å‘³çš„ãªãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ…‹ï¼ˆç›®æ¨™ã‚„æ–¹é‡ãªã©ï¼‰
        self.state_texts = {
            "long_term_goal": {
                "text": "",
                "embedding": None
            }
        }

    # ===== æ•°å€¤çŠ¶æ…‹ã®æ“ä½œ =====

    def modify_state_value(self, name, value):
        """æ•°å€¤æƒ…å ±ã‚’æ›´æ–°"""
        if name in self.state_values:
            self.state_values[name] = np.clip(value, -1, 1)
            print(f"ğŸ”„ State æ›´æ–°: {name} = {self.state_values[name]}")
        else:
            print(f"âš ï¸ `{name}` ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

    def get_named_state(self):
        """å…¨æ•°å€¤çŠ¶æ…‹ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™"""
        return self.state_values.copy()

    def update_all_states(self):
        """VAE ã«æ•°å€¤çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        input_tensor = torch.tensor(
            list(self.state_values.values()), dtype=torch.float32
        ).unsqueeze(0)

        _, mu, logvar = self.vae(input_tensor)
        self.state_vector = self.vae.reparameterize(mu, logvar)

    def get_internal_state(self):
        """æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ï¼ˆè©•ä¾¡ç”¨ï¼‰"""
        return self.state_vector.detach().numpy().flatten()

    # ===== ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ…‹ã®æ“ä½œ =====

    def modify_state_text(self, name, text):
        """æ„å‘³çš„ãªãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æ›´æ–°ï¼ˆembeddingå«ã‚€ï¼‰"""
        embedding = self.sentence_transformer.encode(text)
        self.state_texts[name] = {
            "text": text,
            "embedding": embedding
        }
        print(f"ğŸ“ Text State æ›´æ–°: {name} = '{text}'")

    def retrieve_text_state(self, name):
        """ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ…‹ã‚’å–å¾—"""
        if name not in self.state_texts or not self.state_texts[name]["text"]:
            print(f"âš ï¸ `{name}` ã«ãƒ†ã‚­ã‚¹ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        return self.state_texts[name]["text"]

    def get_text_embedding(self, name):
        """ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ…‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        if name not in self.state_texts or self.state_texts[name]["embedding"] is None:
            print(f"âš ï¸ `{name}` ã«ãƒ™ã‚¯ãƒˆãƒ«ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        return self.state_texts[name]["embedding"]
